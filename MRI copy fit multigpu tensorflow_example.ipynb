{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-09 19:14:52.136102: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-09 19:14:52.310611: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-09 19:14:53.141635: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jyuan@Okta-eitm.org/anaconda3/envs/tf/lib/\n","2024-05-09 19:14:53.141746: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jyuan@Okta-eitm.org/anaconda3/envs/tf/lib/\n","2024-05-09 19:14:53.141757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["import tensorflow as tf\n","import keras"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-09 19:15:34.463913: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-09 19:15:36.732638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10783 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7\n","2024-05-09 19:15:36.734030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10783 MB memory:  -> device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7\n","2024-05-09 19:15:36.735308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10783 MB memory:  -> device: 2, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7\n","2024-05-09 19:15:36.736584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10783 MB memory:  -> device: 3, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n","Number of devices: 4\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 1s 0us/step\n"]},{"name":"stderr","output_type":"stream","text":["2024-05-09 19:15:39.814992: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n","op: \"TensorSliceDataset\"\n","input: \"Placeholder/_0\"\n","input: \"Placeholder/_1\"\n","attr {\n","  key: \"Toutput_types\"\n","  value {\n","    list {\n","      type: DT_FLOAT\n","      type: DT_FLOAT\n","    }\n","  }\n","}\n","attr {\n","  key: \"_cardinality\"\n","  value {\n","    i: 50000\n","  }\n","}\n","attr {\n","  key: \"is_files\"\n","  value {\n","    b: false\n","  }\n","}\n","attr {\n","  key: \"metadata\"\n","  value {\n","    s: \"\\n\\024TensorSliceDataset:0\"\n","  }\n","}\n","attr {\n","  key: \"output_shapes\"\n","  value {\n","    list {\n","      shape {\n","        dim {\n","          size: 784\n","        }\n","      }\n","      shape {\n","      }\n","    }\n","  }\n","}\n","attr {\n","  key: \"replicate_on_split\"\n","  value {\n","    b: false\n","  }\n","}\n","experimental_type {\n","  type_id: TFT_PRODUCT\n","  args {\n","    type_id: TFT_DATASET\n","    args {\n","      type_id: TFT_PRODUCT\n","      args {\n","        type_id: TFT_TENSOR\n","        args {\n","          type_id: TFT_FLOAT\n","        }\n","      }\n","      args {\n","        type_id: TFT_TENSOR\n","        args {\n","          type_id: TFT_FLOAT\n","        }\n","      }\n","    }\n","  }\n","}\n","\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n","1559/1563 [============================>.] - ETA: 0s - loss: 0.2261 - sparse_categorical_accuracy: 0.9321"]},{"name":"stderr","output_type":"stream","text":["2024-05-09 19:15:56.710239: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n","op: \"TensorSliceDataset\"\n","input: \"Placeholder/_0\"\n","input: \"Placeholder/_1\"\n","attr {\n","  key: \"Toutput_types\"\n","  value {\n","    list {\n","      type: DT_FLOAT\n","      type: DT_FLOAT\n","    }\n","  }\n","}\n","attr {\n","  key: \"_cardinality\"\n","  value {\n","    i: 10000\n","  }\n","}\n","attr {\n","  key: \"is_files\"\n","  value {\n","    b: false\n","  }\n","}\n","attr {\n","  key: \"metadata\"\n","  value {\n","    s: \"\\n\\024TensorSliceDataset:2\"\n","  }\n","}\n","attr {\n","  key: \"output_shapes\"\n","  value {\n","    list {\n","      shape {\n","        dim {\n","          size: 784\n","        }\n","      }\n","      shape {\n","      }\n","    }\n","  }\n","}\n","attr {\n","  key: \"replicate_on_split\"\n","  value {\n","    b: false\n","  }\n","}\n","experimental_type {\n","  type_id: TFT_PRODUCT\n","  args {\n","    type_id: TFT_DATASET\n","    args {\n","      type_id: TFT_PRODUCT\n","      args {\n","        type_id: TFT_TENSOR\n","        args {\n","          type_id: TFT_FLOAT\n","        }\n","      }\n","      args {\n","        type_id: TFT_TENSOR\n","        args {\n","          type_id: TFT_FLOAT\n","        }\n","      }\n","    }\n","  }\n","}\n","\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 20s 9ms/step - loss: 0.2263 - sparse_categorical_accuracy: 0.9321 - val_loss: 0.1287 - val_sparse_categorical_accuracy: 0.9583\n","Epoch 2/2\n","1563/1563 [==============================] - 12s 8ms/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9717 - val_loss: 0.0929 - val_sparse_categorical_accuracy: 0.9712\n"]},{"name":"stderr","output_type":"stream","text":["2024-05-09 19:16:12.759081: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n","op: \"TensorSliceDataset\"\n","input: \"Placeholder/_0\"\n","input: \"Placeholder/_1\"\n","attr {\n","  key: \"Toutput_types\"\n","  value {\n","    list {\n","      type: DT_FLOAT\n","      type: DT_FLOAT\n","    }\n","  }\n","}\n","attr {\n","  key: \"_cardinality\"\n","  value {\n","    i: 10000\n","  }\n","}\n","attr {\n","  key: \"is_files\"\n","  value {\n","    b: false\n","  }\n","}\n","attr {\n","  key: \"metadata\"\n","  value {\n","    s: \"\\n\\024TensorSliceDataset:4\"\n","  }\n","}\n","attr {\n","  key: \"output_shapes\"\n","  value {\n","    list {\n","      shape {\n","        dim {\n","          size: 784\n","        }\n","      }\n","      shape {\n","      }\n","    }\n","  }\n","}\n","attr {\n","  key: \"replicate_on_split\"\n","  value {\n","    b: false\n","  }\n","}\n","experimental_type {\n","  type_id: TFT_PRODUCT\n","  args {\n","    type_id: TFT_DATASET\n","    args {\n","      type_id: TFT_PRODUCT\n","      args {\n","        type_id: TFT_TENSOR\n","        args {\n","          type_id: TFT_FLOAT\n","        }\n","      }\n","      args {\n","        type_id: TFT_TENSOR\n","        args {\n","          type_id: TFT_FLOAT\n","        }\n","      }\n","    }\n","  }\n","}\n","\n"]},{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 2s 5ms/step - loss: 0.0926 - sparse_categorical_accuracy: 0.9716\n"]},{"data":{"text/plain":["[0.09255466610193253, 0.9715999960899353]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["def get_compiled_model():\n","    # Make a simple 2-layer densely-connected neural network.\n","    inputs = keras.Input(shape=(784,))\n","    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n","    x = keras.layers.Dense(256, activation=\"relu\")(x)\n","    outputs = keras.layers.Dense(10)(x)\n","    model = keras.Model(inputs, outputs)\n","    model.compile(\n","        optimizer=keras.optimizers.Adam(),\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n","    )\n","    return model\n","\n","\n","def get_dataset():\n","    batch_size = 32\n","    num_val_samples = 10000\n","\n","    # Return the MNIST dataset in the form of a `tf.data.Dataset`.\n","    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","\n","    # Preprocess the data (these are Numpy arrays)\n","    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\n","    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\n","    y_train = y_train.astype(\"float32\")\n","    y_test = y_test.astype(\"float32\")\n","\n","    # Reserve num_val_samples samples for validation\n","    x_val = x_train[-num_val_samples:]\n","    y_val = y_train[-num_val_samples:]\n","    x_train = x_train[:-num_val_samples]\n","    y_train = y_train[:-num_val_samples]\n","    return (\n","        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\n","        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\n","        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\n","    )\n","\n","\n","# Create a MirroredStrategy.\n","strategy = tf.distribute.MirroredStrategy()\n","print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n","\n","# Open a strategy scope.\n","with strategy.scope():\n","    # Everything that creates variables should be under the strategy scope.\n","    # In general this is only model construction & `compile()`.\n","    model = get_compiled_model()\n","\n","# Train the model on all available devices.\n","train_dataset, val_dataset, test_dataset = get_dataset()\n","model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n","\n","# Test the model on all available devices.\n","model.evaluate(test_dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNuaYx3JZSLvsnp7da+t27S","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}
